// Generated by CoffeeScript 1.12.4
(function() {
  var AWS, DAYS, JSONStream, Metrics, MongoAWS, ObjectId, ReadlineStream, S3S, createStream, db, logger, ref, settings, zlib,
    slice = [].slice;

  settings = require("settings-sharelatex");

  logger = require("logger-sharelatex");

  AWS = require('aws-sdk');

  S3S = require('s3-streams');

  ref = require("./mongojs"), db = ref.db, ObjectId = ref.ObjectId;

  JSONStream = require("JSONStream");

  ReadlineStream = require("byline");

  zlib = require("zlib");

  Metrics = require("metrics-sharelatex");

  DAYS = 24 * 3600 * 1000;

  createStream = function(streamConstructor, project_id, doc_id, pack_id) {
    var AWS_CONFIG;
    AWS_CONFIG = {
      accessKeyId: settings.trackchanges.s3.key,
      secretAccessKey: settings.trackchanges.s3.secret,
      endpoint: settings.trackchanges.s3.endpoint,
      s3ForcePathStyle: settings.trackchanges.s3.pathStyle
    };
    return streamConstructor(new AWS.S3(AWS_CONFIG), {
      "Bucket": settings.trackchanges.stores.doc_history,
      "Key": project_id + "/changes-" + doc_id + "/pack-" + pack_id
    });
  };

  module.exports = MongoAWS = {
    archivePack: function(project_id, doc_id, pack_id, _callback) {
      var callback, query, upload;
      if (_callback == null) {
        _callback = function(error) {};
      }
      callback = function() {
        var args;
        args = 1 <= arguments.length ? slice.call(arguments, 0) : [];
        _callback.apply(null, args);
        return _callback = function() {};
      };
      query = {
        _id: ObjectId(pack_id),
        doc_id: ObjectId(doc_id)
      };
      if (project_id == null) {
        return callback(new Error("invalid project id"));
      }
      if (doc_id == null) {
        return callback(new Error("invalid doc id"));
      }
      if (pack_id == null) {
        return callback(new Error("invalid pack id"));
      }
      logger.log({
        project_id: project_id,
        doc_id: doc_id,
        pack_id: pack_id
      }, "uploading data to s3");
      upload = createStream(S3S.WriteStream, project_id, doc_id, pack_id);
      return db.docHistory.findOne(query, function(err, result) {
        var error, uncompressedData;
        if (err != null) {
          return callback(err);
        }
        if (result == null) {
          return callback(new Error("cannot find pack to send to s3"));
        }
        if (result.expiresAt != null) {
          return callback(new Error("refusing to send pack with TTL to s3"));
        }
        uncompressedData = JSON.stringify(result);
        if (uncompressedData.indexOf("\u0000") !== -1) {
          error = new Error("null bytes found in upload");
          logger.error({
            err: error,
            project_id: project_id,
            doc_id: doc_id,
            pack_id: pack_id
          }, error.message);
          return callback(error);
        }
        return zlib.gzip(uncompressedData, function(err, buf) {
          logger.log({
            project_id: project_id,
            doc_id: doc_id,
            pack_id: pack_id,
            origSize: uncompressedData.length,
            newSize: buf.length
          }, "compressed pack");
          if (err != null) {
            return callback(err);
          }
          upload.on('error', function(err) {
            return callback(err);
          });
          upload.on('finish', function() {
            Metrics.inc("archive-pack");
            logger.log({
              project_id: project_id,
              doc_id: doc_id,
              pack_id: pack_id
            }, "upload to s3 completed");
            return callback(null);
          });
          upload.write(buf);
          return upload.end();
        });
      });
    },
    readArchivedPack: function(project_id, doc_id, pack_id, _callback) {
      var callback, download, gunzip, inputStream, outputStream, parts;
      if (_callback == null) {
        _callback = function(error, result) {};
      }
      callback = function() {
        var args;
        args = 1 <= arguments.length ? slice.call(arguments, 0) : [];
        _callback.apply(null, args);
        return _callback = function() {};
      };
      if (project_id == null) {
        return callback(new Error("invalid project id"));
      }
      if (doc_id == null) {
        return callback(new Error("invalid doc id"));
      }
      if (pack_id == null) {
        return callback(new Error("invalid pack id"));
      }
      logger.log({
        project_id: project_id,
        doc_id: doc_id,
        pack_id: pack_id
      }, "downloading data from s3");
      download = createStream(S3S.ReadStream, project_id, doc_id, pack_id);
      inputStream = download.on('open', function(obj) {
        return 1;
      }).on('error', function(err) {
        return callback(err);
      });
      gunzip = zlib.createGunzip();
      gunzip.setEncoding('utf8');
      gunzip.on('error', function(err) {
        logger.log({
          project_id: project_id,
          doc_id: doc_id,
          pack_id: pack_id,
          err: err
        }, "error uncompressing gzip stream");
        return callback(err);
      });
      outputStream = inputStream.pipe(gunzip);
      parts = [];
      outputStream.on('error', function(err) {
        return callback(err);
      });
      outputStream.on('end', function() {
        var e, i, len, object, op, ref1;
        logger.log({
          project_id: project_id,
          doc_id: doc_id,
          pack_id: pack_id
        }, "download from s3 completed");
        try {
          object = JSON.parse(parts.join(''));
        } catch (error1) {
          e = error1;
          return callback(e);
        }
        object._id = ObjectId(object._id);
        object.doc_id = ObjectId(object.doc_id);
        object.project_id = ObjectId(object.project_id);
        ref1 = object.pack;
        for (i = 0, len = ref1.length; i < len; i++) {
          op = ref1[i];
          if (op._id != null) {
            op._id = ObjectId(op._id);
          }
        }
        return callback(null, object);
      });
      return outputStream.on('data', function(data) {
        return parts.push(data);
      });
    },
    unArchivePack: function(project_id, doc_id, pack_id, callback) {
      if (callback == null) {
        callback = function(error) {};
      }
      return MongoAWS.readArchivedPack(project_id, doc_id, pack_id, function(err, object) {
        if (err != null) {
          return callback(err);
        }
        Metrics.inc("unarchive-pack");
        object.expiresAt = new Date(Date.now() + 7 * DAYS);
        logger.log({
          project_id: project_id,
          doc_id: doc_id,
          pack_id: pack_id
        }, "inserting object from s3");
        return db.docHistory.insert(object, callback);
      });
    }
  };

}).call(this);

//# sourceMappingURL=MongoAWS.js.map
